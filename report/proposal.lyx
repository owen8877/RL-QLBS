#LyX 2.3 created this file. For more info see http://www.lyx.org/
\lyxformat 544
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass article
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype false
\use_dash_ligatures true
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\float_placement H
\paperfontsize default
\spacing onehalf
\use_hyperref true
\pdf_bookmarks true
\pdf_bookmarksnumbered false
\pdf_bookmarksopen false
\pdf_bookmarksopenlevel 1
\pdf_breaklinks false
\pdf_pdfborder false
\pdf_colorlinks false
\pdf_backref false
\pdf_pdfusetitle true
\papersize default
\use_geometry true
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\use_minted 0
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 3cm
\topmargin 3.5cm
\rightmargin 3cm
\bottommargin 3.5cm
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\is_math_indent 0
\math_numbering_side default
\quotes_style english
\dynamic_quotes 0
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Standard
\begin_inset CommandInset include
LatexCommand include
filename "math_shorthand.lyx"

\end_inset


\end_layout

\begin_layout Title
Proposal of 
\begin_inset Quotes eld
\end_inset

RLOP: Reinforcement Learning in Option Pricing
\begin_inset Quotes erd
\end_inset


\end_layout

\begin_layout Author
Ziheng Chen, Zhou Fang
\end_layout

\begin_layout Section
Introduction
\end_layout

\begin_layout Standard
In this project, we are going to price options by using reinforcement learning.
 To achieve this goal, we need to replicate the payoff of options at different
 times and under different situations by holding and selling the underlying
 stocks at different times and under different situations.
 Since the whole portfolio will be self-financing, and at each step, different
 hedging strategies will make different value trajectories.
 In other words, to price options, people need to hedge a short position
 of the options.
 Therefore, our question is a sequential problem.
 
\end_layout

\begin_layout Subsection
Black-Scholes Model
\end_layout

\begin_layout Standard
Black-Scholes formula (B-S formula) is the first formula that tells people
 how to price European-style call options.
 The idea behind the B-S formula is that one can hedge a short position
 of call options of a certain stock by holding that stock and putting extra
 money at the bank to earn interest; see 
\begin_inset CommandInset citation
LatexCommand cite
key "shreve2004stochastic,alma991057973644906011"
literal "false"

\end_inset

 for more in-depth theory and computation.
 The problem with the B-S formula is people need to hedge short positions
 continuously, which means they should sell or buy stock, and put or borrow
 money from the bank at every moment.
 This is unrealistic, and costly for almost all investors.
 Most importantly, the prices given by the B-S formula differ greatly from
 than prices of options in real life.
 In addition, a sub-optimal hedging position at the current period will
 affect the cash and stock position in the next period.
 The accumulation of those errors will affect the final price profoundly.
\end_layout

\begin_layout Standard
[How discrete time can add another extra offset to the model]
\end_layout

\begin_layout Subsection
Literature Review
\end_layout

\begin_layout Standard
There have been vast works and literatures on option pricing.
 A few exercising strategies for American option are reviewed in 
\begin_inset CommandInset citation
LatexCommand cite
key "li2009learning"
literal "false"

\end_inset

.
 The state space is composed of asset price trajectories 
\begin_inset Formula $\left(S_{0},S_{1},\dots,S_{T}\right)$
\end_inset

 and an absorbing state 
\begin_inset Formula $\boldsymbol{e}$
\end_inset

 which serves as the destination after the option has been exercised.
 The action space simply contains two actions: hold and exercise.
 The only non-zero reward is given when the option has been exercised at
 the value specified by the option.
 The Q-function is derived as in classical RL problems and it is the quantity
 of interest.
 The first method, LSPI (LS policy iteration), combines LSTD (LS with TD
 update) and policy iteration together to achieve efficient learning.
 The second and third ones, FQI (fitted Q-iteration algorithm) and LSMC
 (Least squares Monte Carlo), take an DP approach where the exact exercising
 problem is solved at each trading moment; the only difference lies in that
 FDI takes a forward view in time while LSMC is backward, starting from
 the exercising time.
 In the same spirit, 
\begin_inset CommandInset citation
LatexCommand cite
key "fathan2021deep"
literal "false"

\end_inset

 investigates the efficiency and effectiveness of different parametrizaiton
 structures.
 Three algorithms, namely double deep Q-Learning (DDQN), categorical distributio
nal RL (C51), and implicit quantile networks (IQN) are compared where DDQN
 learns the optimal Q-value function while the other two algorithms try
 to learn the full distribution of the discounted reward.
 They are tested on empirical data as well as simulated geometric Brownian
 motion trajectories.
\end_layout

\begin_layout Standard
Different RL architectures can be deployed in this field as well.
 The Actor-Critic structure is used in 
\begin_inset CommandInset citation
LatexCommand cite
key "marzban2021deep"
literal "false"

\end_inset

 for Equal Risk Pricing (ERP) in a risk averse setting under the framework
 studied in 
\begin_inset CommandInset citation
LatexCommand cite
key "tamar2015policy"
literal "false"

\end_inset

.
 The concept of 
\begin_inset Formula $\tau$
\end_inset

-expectile 
\begin_inset Formula 
\[
\overline{\rho}\left(X\right)=\arg\min_{q}\tau\bE\left[\left(q-X\right)_{+}^{2}\right]+\left(1-\tau\right)\bE\left[\left(q-X\right)_{-}^{2}\right]
\]

\end_inset

is used to elicit a coherent risk measure.
 The value function is defined as the portfolio value under the recursive
 coherent risk measure realized by expectiles, i.e.
\begin_inset Formula 
\[
V_{t}\left(S_{t},Y_{t}\right)=\inf_{\xi_{t}}\overline{\rho}\left(-\xi_{t}^{T}\Delta S_{t+1}+V_{t+1}\left(S_{t+1},Y_{t+1}\right)|S_{t},Y_{t}\right)
\]

\end_inset

with terminal condition 
\begin_inset Formula $V_{T}\left(S_{T},Y_{T}\right)=F\left(S_{T},Y_{T}\right)$
\end_inset

 specified by the option contract.
 With that being established, we can apply the policy gradient method where
 the critic network updates the value estimate which the actor network can
 refer to and build its policy upon.
 The network uses a classical fully multilayer structure with alternating
 activation functions.
\end_layout

\begin_layout Standard
A hybrid attempt is carried out in 
\begin_inset CommandInset citation
LatexCommand cite
key "grassl2010reinforcement"
literal "false"

\end_inset

 where the pricing strategy is based on a combination of optimal stopping
 and terminal payoff.
 The idea is that the agent can either hold the derivative until the terminal
 time, executing the contract to get the payoff written, or sell the derivative
 earlier according to the price at that particular moment.
 The reward function is thus simplified defined as the selling/execution
 price if such scenario happens.
 The value function is parameterized by kernel function approximation and
 the algorithm is tested using simulated geometric Brownian paths of an
 European call option.
\end_layout

\begin_layout Standard
We point out that it is also possible to directly solve the HJB equation
 if the associated RL problem is formulated as a control porblem.
 We refer to 
\begin_inset CommandInset citation
LatexCommand cite
key "halperin2021distributional"
literal "false"

\end_inset

 for more details on distributional offline continuous-time RL learning
 algorithms.
\end_layout

\begin_layout Subsection
Trading Cost 
\begin_inset CommandInset label
LatexCommand label
name "subsec:Trading-Cost"

\end_inset


\end_layout

\begin_layout Standard
[TBA]
\end_layout

\begin_layout Section
QLBS: Q-learning Black-Scholes Model
\end_layout

\begin_layout Subsection
A Brief Review
\end_layout

\begin_layout Standard
We quickly introduce the QLBS model proposed in 
\begin_inset CommandInset citation
LatexCommand cite
key "halperin2020qlbs"
literal "false"

\end_inset

.
 Consider a sequence of asset prices 
\begin_inset Formula $\left\{ S_{t}\right\} _{t=0,1,\dots}$
\end_inset

, adapted under the filtration 
\begin_inset Formula $\left\{ \cF_{t}\right\} $
\end_inset

, upon which we wish to build an option with payoff function 
\begin_inset Formula $h$
\end_inset

 at the maturity time 
\begin_inset Formula $T$
\end_inset

.
 The option is realized as a hedge portfolio which consists of some holdings
 
\begin_inset Formula $u_{t}$
\end_inset

 of the underlying asset and the risk-free deposit 
\begin_inset Formula $B_{t}$
\end_inset

.
 Let 
\begin_inset Formula 
\[
\Pi_{t}:=u_{t}S_{t}+B_{t}
\]

\end_inset

denote the value of the portfolio at time 
\begin_inset Formula $t$
\end_inset

.
 To fulfill the option contract, the holding position is cleared at the
 terminal time 
\begin_inset Formula $T$
\end_inset

 and is fully converted into cash position, i.e.
\begin_inset Formula 
\[
\Pi_{T}=B_{T}=h\left(S_{T}\right).
\]

\end_inset

The deposit position at a particular time is solved via the self-financing
 condition which requires that the instantaneous value of the portfolio
 is kept same before and after the re-hedging operation:
\begin_inset Formula 
\begin{equation}
u_{t}S_{t+1}+e^{r\Delta t}B_{t}=u_{t+1}S_{t+1}+B_{t+1}\label{eq:self-financing}
\end{equation}

\end_inset

where 
\begin_inset Formula $r$
\end_inset

 stands for the risk-free interest rate.
 Eqn.
 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:self-financing"

\end_inset

 can be used to solve how much money is needed to cover future trading activitie
s.
\end_layout

\begin_layout Standard
A state is defined as a pair of an integer and a real number 
\begin_inset Formula $\left(t,X_{t}\right)$
\end_inset

, where 
\begin_inset Formula 
\[
X_{t}=-\left(\mu-\frac{\sigma^{2}}{2}\right)t+\log S_{t}
\]

\end_inset

is the compensated logarithm price.
 The action space is the real number, indicating how much shares are hedged
 at a particular time.
 A policy is a mapping from the state space to the action space, i.e.
\begin_inset Formula 
\[
\pi:\left(t,X_{t}\right)\mapsto a_{t}.
\]

\end_inset

Notice that we use 
\begin_inset Formula $a_{t}$
\end_inset

 for the log-processed input 
\begin_inset Formula $X_{t}$
\end_inset

 while 
\begin_inset Formula $u_{t}$
\end_inset

 for the asset price 
\begin_inset Formula $S_{t}$
\end_inset

 in normal scale.
 The policy may depend on other macro factors, e.g.
 interest rate 
\begin_inset Formula $r$
\end_inset

, volatility 
\begin_inset Formula $\sigma$
\end_inset

, total maturity time 
\begin_inset Formula $T$
\end_inset

, the strike price 
\begin_inset Formula $K$
\end_inset

 if the option is of call/put type.
\end_layout

\begin_layout Standard
The reward function is derived from the Bellman's optimality equation where
 we define the value function in the first place.
 The idea is to minimize the money needed to initiate the hedge portfolio
 as well as to minimize the volatility throughout the trading periods.
 Given a hedging strategy 
\begin_inset Formula $\pi$
\end_inset

, the value function is defined as
\begin_inset Formula 
\begin{equation}
V_{t}^{\pi}\left(X_{t}\right)=\bE^{\pi}\left[-\Pi_{t}\left(X_{t}\right)-\lambda\sum_{\tau=t}^{T}e^{-r\left(\tau-t\right)}\text{Var}\left[\Pi_{\tau}\left(X_{\tau}\right)|\cF_{\tau}\right]|\cF_{t}\right]\label{eq:qlbs-value-function}
\end{equation}

\end_inset

where 
\begin_inset Formula $\lambda$
\end_inset

 is the risk aversion factor.
 The reward function can be derived by matching the corresponding terms
 in the Bellman's equation:
\begin_inset Formula 
\[
R_{t}\left(X_{t},a_{t},X_{t+1}\right):=\gamma a_{t}\Delta S_{t}-\lambda\text{Var}\left[\Pi_{t}|\cF_{t}\right]
\]

\end_inset

where 
\begin_inset Formula $\gamma:=e^{-r\Delta t}$
\end_inset

 is the discounting factor.
 The connection between the value function and option pricing is that the
 option price is given by the minus optimal Q-function.
\end_layout

\begin_layout Subsection
QLBS as a RL Model
\end_layout

\begin_layout Standard
Elegant as the vanilla QLBS approach, it is not a truly RL problem since
 the optimal policy 
\begin_inset Formula $\pi^{*}$
\end_inset

 is solved analytically without any reinforcement learning techniques.
 In fact, the author derives the Bellman's equation for the optimal Q-function
 from Eq.
 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:qlbs-value-function"

\end_inset


\begin_inset Formula 
\begin{equation}
Q_{t}^{*}\left(X_{t},a_{t}\right)=\gamma\bE_{t}\left[Q_{t+1}\left(X_{t+1},a_{t+1}^{*}\right)+a_{t}\Delta S_{t}\right]-\lambda\gamma^{2}\bE_{t}\left[\widehat{\Pi}_{t+1}^{2}-2a_{t}\widehat{\Pi}_{t+1}\Delta\widehat{S}_{t}+a_{t}^{2}\left(\Delta\widehat{S}_{t}\right)^{2}\right]\label{eq:qlbs-optimal-Q-function}
\end{equation}

\end_inset

which admits the optimal policy in closed form since Eqn.
 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:qlbs-optimal-Q-function"

\end_inset

 is a quadratic function in 
\begin_inset Formula $a_{t}$
\end_inset

.
 Such direct approach is feasible if provided with abundant data on the
 correlation structure of the portofolio value 
\begin_inset Formula $\widehat{\Pi}$
\end_inset

 and the stock price change 
\begin_inset Formula $\Delta\widehat{S}$
\end_inset

, but it fails to generalize beyond this simple setting.
 Besides, the portofolio value process 
\begin_inset Formula $\Pi_{t}$
\end_inset

 is in general non-adapted due to how the self-financing condition (Eqn.
 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:self-financing"

\end_inset

) works.
 
\end_layout

\begin_layout Standard
To deal with the aforementioned issues, we propose a modified QLBS model
 which is
\end_layout

\begin_layout Enumerate
fully adapted with respect to the given filtration 
\begin_inset Formula $\left\{ \cF_{t}\right\} $
\end_inset

,
\end_layout

\begin_layout Enumerate
compatible with the transaction cost proposed in Sec.
 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:Trading-Cost"

\end_inset

, and
\end_layout

\begin_layout Enumerate
works well with value-based or policy-based learning algorithms.
\end_layout

\begin_layout Standard
We start by modifying the value function.
 The version introduced in Eqn.
 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:qlbs-value-function"

\end_inset

 has the problem that it assigns the (negative) cashflow with a risk part
 as the reward, but the terminal step gets the option payoff which is in
 general much larger than the previous steps.
 Technically speaking, an agent could notice this heterogenity since the
 time 
\begin_inset Formula $t$
\end_inset

 aligns with the terminal time 
\begin_inset Formula $T$
\end_inset

, but it is rather difficult in practice to figure out this situation.
 Thus, we propose the modified value function 
\begin_inset Marginal
status open

\begin_layout Plain Layout
illustration
\end_layout

\end_inset


\begin_inset Formula 
\[
V_{t}^{\pi}\left(X_{t}\right)=\bE_{t}^{\pi}\left[-\left(1-\frac{t}{T}\right)\Pi_{t}\left(X_{t}\right)-\lambda\sum_{\tau=t}^{T}\gamma^{\tau-t}\sqrt{\text{Var}\left[\Pi_{\tau}\left(X_{\tau}\right)\right]}\right].
\]

\end_inset

Our improvement is two folds:
\end_layout

\begin_layout Enumerate
We weight the portfolio term by a diminishing factor 
\begin_inset Formula $\left(1-\frac{t}{T}\right)$
\end_inset

.
 This factor does not impact the starting estimate at 
\begin_inset Formula $t=0$
\end_inset

 and it fully vanishes at 
\begin_inset Formula $t=T$
\end_inset

.
 We point out that introducing this factor will break the temporal symmetry
 so that the intermediate estimate does not correspond to option pricing
 from the intermediate time steps.
\end_layout

\begin_layout Enumerate
We take a square root of the variance terms so that they turn into standard
 deviation.
 This helps to keep the value estimate dimensionless and robust.
\end_layout

\begin_layout Standard
The reward function changes accordingly to
\begin_inset Formula 
\begin{equation}
R_{t+1}\left(X_{t},a_{t}\right)=\bE_{t}^{\pi}\left[-\left(1-\frac{t}{T}\right)\Pi_{t}\left(X_{t}\right)+\left(1-\frac{t+1}{T}\right)\Pi_{t}\left(X_{t}\right)-\lambda\sqrt{\text{Var}\left[\Pi_{t}\left(X_{t}\right)\right]}\right]\label{eq:qlbs-modified-reward}
\end{equation}

\end_inset

with action at time 
\begin_inset Formula $t$
\end_inset

 to be 
\begin_inset Formula $a_{t}$
\end_inset

 and remaining actions following the current policy 
\begin_inset Formula $\pi$
\end_inset

.
 Considering the effect of transaction costs, the portfolio value process
 
\begin_inset Formula $\Pi_{t}$
\end_inset

 is calculated backwards from the modified self-financing condition
\begin_inset Formula 
\begin{equation}
e^{r\Delta t}\left(\Pi_{t}-u_{t}S_{t}\right)+u_{t}S_{t+1}=\Pi_{t+1}+\text{TC}\left(u_{t+1}-u_{t},S_{t+1}\right)\label{eq:qlbs-modified-self-financing}
\end{equation}

\end_inset

with terminal condition 
\begin_inset Formula $\Pi_{T}=h\left(S_{T}\right)$
\end_inset

 unchanged.
 We point out that we wrap the cashflow part with an conditional expectation
 at time 
\begin_inset Formula $t$
\end_inset

 so that we don't run into adaptedness issues.
 We briefly illustrate the modified QLBS model in Fig.
 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:qlbs-illustration"

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename qlbs illustration.png
	width 60line%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
A brief illustration on the modified QLBS environment.
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:qlbs-illustration"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Specification
\end_layout

\begin_layout Standard
In this section, we describe how to set-up the QLBS environment and the
 necessary numerical procedures.
\end_layout

\begin_layout Subsubsection
Environment
\end_layout

\begin_layout Standard
The environment is responsible for keeping track of the asset price and
 portfolio value based on the actions provided by the agent.
 With a given set of parameters 
\begin_inset Formula $r,\mu,\sigma,T$
\end_inset

, the asset prices are a set of geometrical brownian motion paths 
\begin_inset Formula $\left\{ S_{t}\right\} $
\end_inset

 which solves
\begin_inset Formula 
\[
\d{S_{t}}=\mu S_{t}\d t+\sigma S_{t}\d{W_{t}}
\]

\end_inset

where 
\begin_inset Formula $W_{t}$
\end_inset

 refers to the standard brownian motion according to the filtration 
\begin_inset Formula $\left\{ \cF_{t}\right\} $
\end_inset

.
 At each time step 
\begin_inset Formula $t$
\end_inset

, the normalized price 
\begin_inset Formula $X_{t}=-\left(\mu-\frac{\sigma^{2}}{2}\right)t+\log S_{t}$
\end_inset

 and 
\begin_inset Formula $t$
\end_inset

 are provided to the agent, waiting for the response of the hedge position
 
\begin_inset Formula $a_{t}$
\end_inset

.
 Then, the reward 
\begin_inset Formula $R_{t+1}$
\end_inset

, as a conditional expectation specified in Eqn.
 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:qlbs-modified-reward"

\end_inset

, is computed empirically by averaging samples from a fixed number of additional
 trajectories under the current policy.
 At the beginning of each episode, the parameters are adjusted in a random
 fashion to help the agent explore different settings and help avoid overfitting
; the adjustment obeys a Poisson process with intensity 
\begin_inset Formula $\varUpsilon$
\end_inset

.
\end_layout

\begin_layout Subsubsection
Agent Parametrization
\end_layout

\begin_layout Standard
The agent is fully responsible for determining the hedge position 
\begin_inset Formula $a_{t}$
\end_inset

 under a given normalized price at each time step.
 The policy 
\begin_inset Formula $\pi$
\end_inset

, whether stochastic or deterministic, depends on these input variables
 as well as the environment parameters
\begin_inset Formula 
\[
a_{t}\sim\pi\left(X_{t},t;r,\mu,\sigma,T,K,\lambda\right).
\]

\end_inset

In pratice, we prefer a stochastic policy since it encourages exploration
 which is helpful to excape local minima.
 To further simplify the sampling procedure, we restrict our policy spaces
 to Gaussian distribution where the agent determines the mean and standard
 deviation, i.e.
\begin_inset Formula 
\[
\pi=\cN\left(\mu_{\pi},\sigma_{\pi}\right)
\]

\end_inset

(where the subscripts 
\begin_inset Formula $\pi$
\end_inset

 are used to distinguish these parameters from the environmental ones).
 The statistics 
\begin_inset Formula $\mu_{\pi}$
\end_inset

 and 
\begin_inset Formula $\sigma_{\pi}$
\end_inset

 are parametrized by two separate neural networks with the Resnet skip-connectio
n 
\begin_inset Marginal
status open

\begin_layout Plain Layout
ref
\end_layout

\end_inset

 structure.
 
\begin_inset Marginal
status open

\begin_layout Plain Layout
Illustration on resnet
\end_layout

\end_inset

 The Resnet structure is composed of three parts:
\end_layout

\begin_layout Enumerate
Pre-processing 
\begin_inset Formula $T_{\text{lift}}\left(x\right):=\Xi\left(w_{\text{lift}}^{T}x+b_{\text{lift}}\right)$
\end_inset

, that lifts the (8-dimensional) input to the latent dimension by an affine
 transform and ac activation funciton 
\begin_inset Formula $\Xi$
\end_inset

;
\end_layout

\begin_layout Enumerate
Chain of transforms 
\begin_inset Formula $\left\{ T^{\left(k\right)}\right\} $
\end_inset

 in a fixed-point iteration style, with each transform combining the identity
 functions and a series of alternating affine transforms 
\begin_inset Formula $\left\{ Z_{l}^{\left(k\right)}\right\} $
\end_inset

 and activation 
\begin_inset Formula $\Xi$
\end_inset

, i.e.
\begin_inset Formula 
\[
T^{\left(k\right)}:=\Xi\circ\left[\text{id}+Z_{n_{k}}^{\left(k\right)}\circ\Xi\circ Z_{n_{k}-1}^{\left(k\right)}\circ\cdots\circ\Xi\circ Z_{1}^{\left(k\right)}\right]
\]

\end_inset


\end_layout

\begin_layout Enumerate
Post-processing 
\begin_inset Formula $T_{\text{project}}\left(x\right):=w_{\text{project}}^{T}x+b_{\text{project}}$
\end_inset

, that projects the latent representations onto the target space.
\end_layout

\begin_layout Standard
Thus, the Resnet realization can be formally written as 
\begin_inset Formula $T_{\text{project}}\circ T^{\left(k\right)}\circ T^{\left(k-1\right)}\circ\cdots\circ T^{\left(1\right)}\circ T_{\text{lift}}$
\end_inset

.
\end_layout

\begin_layout Subsubsection
Learning Algorithm
\end_layout

\begin_layout Standard
Since the action space 
\begin_inset Formula $a_{t}\in\bR$
\end_inset

 is a continuous space, it is nature to adopt a policy-based method.
 Here we opt-in the classical Critic-Actor model where the policy components
 relies on the value estimator to learn quickly and reliably while the value
 estimator learns from empirical averages of Monte Carlo samples.
 The value estimator, parameterized by a neural network, uses the same Resnet
 structure as mentioned in the previous section.
 We refer to the REINFORCE algorithm 
\begin_inset Marginal
status open

\begin_layout Plain Layout
ref?
\end_layout

\end_inset

 for details.
\end_layout

\begin_layout Standard
In our implementation, the policy network and the value network have the
 same latent dimension 10 and they are composed of two Resnet blocks with
 two hidden affine transforms.
 We use the Adam optimizers to update the networks, one for each.
\end_layout

\begin_layout Subsection
Experiments and Results
\end_layout

\begin_layout Standard
We include a variety of experiments, starting from a demonstration that
 shows the agent learns over time, moving to a comparison with the Black-Scholes
 baseline model, and finally exploring other directions that have a stronger
 connection to finance.
\end_layout

\begin_layout Subsubsection
Demonstration on learning
\begin_inset CommandInset label
LatexCommand label
name "subsec:Demonstration-on-learning"

\end_inset


\end_layout

\begin_layout Standard
We start by showing that the policy gradient algorithm has been correctly
 implemented and the agent learns well over time.
 The environment parameter is set to 
\begin_inset Formula $r=0.01,\sigma=0.1,\mu=0,T=5,K=1,S_{0}=1,\Delta t=1,\lambda\in\left\{ 0,1,2,3\right\} ,\epsilon=0$
\end_inset

 in Sec.
 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:Demonstration-on-learning"

\end_inset

, 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:Influence-of-risk"

\end_inset

, and 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:Effect-of-transaction"

\end_inset

 unless otherwise specified.
 As shown in Fig.
 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:qlbs-exp1-learning"

\end_inset

, the episodic return flatterns after training for around 3000 steps.
 It is worth noticing that the cashflow return part 
\begin_inset Formula 
\[
\sum_{t=0}^{T-1}\bE^{\pi}\left[-\left(1-\frac{t}{T}\right)\Pi_{t}\left(X_{t}\right)+\left(1-\frac{t+1}{T}\right)\Pi_{t}\left(X_{t}\right)\right]=\bE^{\pi}\left[\Pi_{0}\left(X_{0}\right)\right]
\]

\end_inset

is slightly smaller for a large risk parameter 
\begin_inset Formula $\lambda$
\end_inset

, implying that the agent learns to make a trade-off between the cash-flow
 and the risk component.
 
\end_layout

\begin_layout Standard
To prepare for learning in a much broader setting, we allow the initial
 price to be adjusted at a given intensity 
\begin_inset Formula $\varUpsilon$
\end_inset

.
 We mark out the episodes that the adjustments take place (Fig.
 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:qlbs-exp1-mutate"

\end_inset

) and it seems that the agent can swiftly adapt to the change.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\begin_inset Graphics
	filename qlbs/experiment1/learning-curve.png
	width 45line%

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
No adjustment on environment parameters.
 The shade indicates 95% confidence interval.
 Learning rate set to 
\begin_inset Formula $10^{-4}$
\end_inset

.
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:qlbs-exp1-learning"

\end_inset


\end_layout

\end_inset


\begin_inset space \qquad{}
\end_inset


\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\begin_inset Graphics
	filename qlbs/experiment1/mutate.png
	width 45line%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Adjustment parameter (on the initial price) 
\begin_inset Formula $\varUpsilon=0.005$
\end_inset

, indicated by purple lines.
 The shade indicates 95% confidence interval.
 Learning rate set to 
\begin_inset Formula $10^{-4}$
\end_inset

.
 
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:qlbs-exp1-mutate"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Exponentially moving average of episodic return.
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:qlbs-exp1"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsubsection
Influence of risk parameter 
\begin_inset Formula $\lambda$
\end_inset


\begin_inset CommandInset label
LatexCommand label
name "subsec:Influence-of-risk"

\end_inset


\end_layout

\begin_layout Standard
We wish to compare the optimal price under different choices of the risk
 aversion parameter 
\begin_inset Formula $\lambda$
\end_inset

.
 A priori estimate is that a larger 
\begin_inset Formula $\lambda$
\end_inset

 leads to a higher price learnt, since it penalizes improper hedges harder.
 To complete the comparision, we also introduce the vanilla BS hedging strategy
 
\begin_inset Formula $\pi_{\text{BS}}$
\end_inset

 and learns the episodic return using the same neural network parametrization.
 The results are shown in Fig.
 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:qlbs-exp2"

\end_inset

.
 We remind the readers that the option prices are defined as the negative
 value function, so a lower option price implies a larger return learnt.
 As we expect, a large 
\begin_inset Formula $\lambda$
\end_inset

 does lead to a higher price, both under BS policy and under the learnt
 policy.
 In general, the policy learnt by the neural network parametrization is
 a bit sub-optimal compared to the BS policy, but it performs better when
 
\begin_inset Formula $\lambda=0.$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename qlbs/experiment2/option-price.png
	width 75line%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Negative episodic returns as option prices.
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:qlbs-exp2"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsubsection
Effect of transaction cost
\begin_inset CommandInset label
LatexCommand label
name "subsec:Effect-of-transaction"

\end_inset


\end_layout

\begin_layout Standard
Next, we'd like to examine the effect of transaction costs.
 As indicated by the self-financing condition (Eqn.
 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:qlbs-modified-self-financing"

\end_inset

), a higher trading cost can significantly bias the portfolio value process
 and consequently increase the optimal price learnt.
 As shown in Fig.
 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:qlbs-exp3"

\end_inset

, we compare the price and hedge position learnt under different friction
 parameters 
\begin_inset Formula $\epsilon$
\end_inset

.
 In general, a larger 
\begin_inset Formula $\epsilon$
\end_inset

 does lead to a higher price learned (shown in the left half) and the hedge
 position is usually always higher (in the right half).
 [Some rationale from financial point of view]
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename qlbs/experiment3/price-hedge.png
	width 100line%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Option priced and hedged under different transaction cost parameters 
\begin_inset Formula $\epsilon$
\end_inset

.
 
\begin_inset Formula $\lambda$
\end_inset

 is set to 0.5 to introduce some risk concerns.
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:qlbs-exp3"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsubsection
Generalization power
\end_layout

\begin_layout Standard
As a concluding setting, we wish to parametrize the policy and baseline
 networks on not only the states but also the environment parameters and
 examine the generalization power.
 We pick two set of parameters: 
\begin_inset Formula $r=0.01,\mu=0,\sigma=0.1,\lambda=0.5,\epsilon=0$
\end_inset

 under the first condition while 
\begin_inset Formula $r=0.02,\mu=0.1,\sigma=0.2,\lambda=1.5,\epsilon=0.1$
\end_inset

 under the second condition.
 We train a policy and baseline under these two conditions mixed, i.e.
 they have the same probability of being presented, with the switching process
 being a Poisson process.
 After training for a while, we refine the agent to learn a third condition
 where the parameters are set to the arithmic average of the given two condition
s.
 As shown in Fig.
 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:qlbs-exp4"

\end_inset

, we examine the hedging strategy of the agent before and after fine tuning
 to the third average condition.
 The result, that the fine tuning does a great improvement, is not surprising
 since we don't expect a lot of generalization power.
 However, one could expect such generalization given enough amount of training
 time.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename qlbs/experiment4/price-hedge.png
	width 100line%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Option priced and hedged under different conditions.
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:qlbs-exp4"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Section
RLOP: Replication Learning of Option Pricing
\end_layout

\begin_layout Standard
In this section, we propose a novel algorithm that prices a call/put option
 via portfolio replication, but this method uses a forward view which is
 fundamentally different from the QLBS approach.
 The idea is simple: the agent manages a portfolio which yields a reward
 at the terminal time based on how accurate the portfolio value is compared
 to the option payoff.
 This naive idea has the problem that the reward is zero for quite a long
 time until the maturity, which is usually not good for shaping the agent's
 behavior.
 To deal with this downside, we propose to group a few options as an ensemble
 so that the agent gets a stream of feedbacks during each episode.
 To be specific, given a (simulated or historical) path of the asset price
 
\begin_inset Formula $\left\{ S_{t}\right\} $
\end_inset

, maturity time 
\begin_inset Formula $T$
\end_inset

, and the payoff function 
\begin_inset Formula $h$
\end_inset

, the agent needs to manages (at most) 
\begin_inset Formula $T$
\end_inset

 portfolios 
\begin_inset Formula $\left\{ \Pi_{t}^{\left(i\right)}\right\} _{i=1}^{T}$
\end_inset

 at the same time, where the 
\begin_inset Formula $k$
\end_inset

-th portfolio replicates the option that terminals at time step 
\begin_inset Formula $k$
\end_inset

.
 In other words, for every 
\begin_inset Formula $i\in\left[t+1,T\right]$
\end_inset

, the agent proposes the hedge position 
\begin_inset Formula $u_{t}^{\left(i\right)}$
\end_inset

 based on the current time step 
\begin_inset Formula $t$
\end_inset

, the asset price 
\begin_inset Formula $S_{t}$
\end_inset

, and the balance of the 
\begin_inset Formula $i$
\end_inset

-th portfolio 
\begin_inset Formula $\Pi_{t}^{\left(i\right)}$
\end_inset

.
 We hold the belief that the agent is able to learn the hedging strategy
 step by step from small 
\begin_inset Formula $t$
\end_inset

 to the terminal time 
\begin_inset Formula $T$
\end_inset

.
\end_layout

\begin_layout Subsection
MDP Formulation
\end_layout

\begin_layout Standard
We now rigorously define the aforementioned problem as a MDP.
 Given a maturity time 
\begin_inset Formula $i$
\end_inset

, the state space consists of tuples containing the time step 
\begin_inset Formula $t$
\end_inset

, the asset price 
\begin_inset Formula $S_{t}$
\end_inset

, and the current portfolio value 
\begin_inset Formula $\Pi_{t}^{\left(i\right)}$
\end_inset

.
 The action space is 
\begin_inset Formula $\bR$
\end_inset

 that contains all possible hedging positions.
 The transitional probability (density) function is defined as
\begin_inset Formula 
\[
p\left(\left(t,S_{t},\Pi_{t}^{\left(i\right)}\right),u_{t}^{\left(i\right)}\to\left(t',S_{t'},\Pi_{t'}^{\left(i\right)}\right),R_{t+1}\right)=\begin{cases}
\delta_{t+1,t'}\delta_{\widetilde{\Pi}_{t+1}^{\left(i\right)},\Pi_{t'}^{\left(i\right)}}\rho\left(S_{t},S_{t'}\right) & t<i\\
0 & t=i
\end{cases}
\]

\end_inset

where 
\end_layout

\begin_layout Itemize
the only admissible state is the terminal state when 
\begin_inset Formula $t>i$
\end_inset

, marking the end of this episode;
\end_layout

\begin_layout Itemize
\begin_inset Formula $\widetilde{\Pi}_{t+1}^{\left(i\right)}$
\end_inset

 refers to the portfolio value determined by the self-financing condition
\begin_inset Formula 
\[
\widetilde{\Pi}_{t+1}^{\left(i\right)}=e^{r\Delta t}\left(\Pi_{t}-u_{t}^{\left(i\right)}S_{t}\right)+u_{t}^{\left(i\right)}S_{t+1}-\text{TC}\left(u_{t+1}^{\left(i\right)}-u_{t}^{\left(i\right)},S_{t+1}\right)
\]

\end_inset

(which is the same as Eqn.
 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:qlbs-modified-self-financing"

\end_inset

);
\end_layout

\begin_layout Itemize
\begin_inset Formula $\rho$
\end_inset

 characterizes the dynamics of the underlying asset, e.g.
 the discrete version geometric brownian motion;
\end_layout

\begin_layout Itemize
\begin_inset Formula $R_{t+1}=0$
\end_inset

 for 
\begin_inset Formula $t+1<i$
\end_inset

 and 
\begin_inset Formula $R_{i}=H\left(h\left(S_{i}\right),\Pi_{i}^{\left(i\right)}\right)$
\end_inset

 where 
\begin_inset Formula $H$
\end_inset

 is a given penalty function that measures how accurate the portfolio value
 
\begin_inset Formula $\Pi_{i}^{\left(i\right)}$
\end_inset

 mimics the option payoff 
\begin_inset Formula $h\left(S_{i}\right)$
\end_inset

.
\end_layout

\begin_layout Standard
As we mentioned in the previous paragraph, we stack a few....
 We briefly illustrate the stucture of RLOP in Fig.
 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:rlop-illustration"

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename rlop illustration.png
	width 60line%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
A brief illustration on the modified RLOP environment.
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:rlop-illustration"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Specification
\end_layout

\begin_layout Standard
In this section, we describe how to set-up the RLOP environment and the
 necessary numerical procedures.
\end_layout

\begin_layout Subsubsection
Environment
\end_layout

\begin_layout Standard
...
\end_layout

\begin_layout Subsubsection
Agent Parametrization
\end_layout

\begin_layout Standard
...
\end_layout

\begin_layout Subsubsection
Learning Algorithm
\end_layout

\begin_layout Subsection
Experiments and Results
\end_layout

\begin_layout Standard
We include a variety of experiments, including a demonstration that shows
 the agent learns over time, a comparison with the Black-Scholes baseline
 model, and a another one on the effect of trading costs.
\end_layout

\begin_layout Subsubsection
Demonstration on learning
\begin_inset CommandInset label
LatexCommand label
name "subsec:Demonstration-on-learning-rlop"

\end_inset


\end_layout

\begin_layout Standard
We use a parameter setting similar to Sec.
 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:Demonstration-on-learning"

\end_inset

 where 
\begin_inset Formula $r=0.01,\sigma=0.1,\mu=0,T=5,K=1,S_{0}=1,\Delta t=1$
\end_inset

.
 The agent is trained under no transaction cost with a fixed initial asset
 value and the training curve is flat after around 8000 steps, shown in
 Fig.
 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:rlop-exp1-learns"

\end_inset

.
 Similar to the QLBS setting, we introduce adjustment to the parameters
 (i.e.
 the initial asset value in the experiments to follow) by a Poisson process.
 We mark where the adjustment occurs in the bottom half of the figure by
 purple triangles.
\end_layout

\begin_layout Standard
We also compare the optimal hedging strategy learnt with the BS predictions.
 In general, the hedging curve should have a greater slope for a smaller
 remaining time which implies that the price of the option is more sensitive
 to the underlying asset price.
 As expected, both the learnt position and the BS position show a greater
 slope when approaching the terminal time.
 The one learnt by our RL agent is slightly insensitve to the time variable.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\begin_inset Graphics
	filename rlop/experiment1/learning-curve.png
	width 47line%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Episodic return improves during training.
 Top figure: no adjustment on the initial price 
\begin_inset Formula $S_{0}$
\end_inset

; bottom figure: the initial price is adjusted via a Poisson process.
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:rlop-exp1-learns"

\end_inset


\end_layout

\end_inset


\begin_inset space \qquad{}
\end_inset


\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename rlop/experiment1/hedge_position.png
	width 47line%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Optimal hedge position learnt compared with BS predictions (Eqn.
 ?).
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:rlop-exp1-hedge"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Demonstration the RLOP model learning curve and a comparison to the BS predictio
ns.
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:rlop-exp1"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsubsection
Effect of transaction cost
\begin_inset CommandInset label
LatexCommand label
name "subsec:Effect-of-transaction-rlop"

\end_inset


\end_layout

\begin_layout Standard
We proceed to measure how the transaction cost can effect the optimal hedging
 strategy.
 We use the same setting as laid out in Sec.
 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:Effect-of-transaction"

\end_inset

 where 
\begin_inset Formula $\epsilon$
\end_inset

 is the friction parameter.
 We plot the optimal hedging position learnt under different 
\begin_inset Formula $\epsilon$
\end_inset

 in Fig.
 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:rlop-exp2"

\end_inset

.
 The general trend is that a large 
\begin_inset Formula $\epsilon$
\end_inset

 discourages hedging amounts, since it leads to a higher trading cost on
 average.
 A higher friction also leads to a larger distinction between positions
 at different times, which implies that in an illuquid market, we need to
 value the time variable more than in a liquid one.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename rlop/experiment2/hedge_position.png
	width 100line%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Optimal hedging strategy learnt under different transaction costs.
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:rlop-exp2"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Section
Conclusion and Discussion
\end_layout

\begin_layout Standard
Based on the aforementioned settings proposed by QLBS 
\begin_inset CommandInset citation
LatexCommand cite
key "halperin2020qlbs"
literal "false"

\end_inset

, we seek to build a method where the RL agent can learn an optimal hedging
 strategy in a realistic setting.
 (We need to point out that QLBS is not an interactive RL model since the
 optimal hedging strategy 
\begin_inset Formula $\pi^{*}$
\end_inset

 is solved in a closed form rather than learning from the reward feedback).
 The initial plan is as follows:
\end_layout

\begin_layout Enumerate
Understand the financial background as well as finalize the mathematical
 standing point.
\end_layout

\begin_layout Enumerate
Implement a hedging environment as well as collect data for training purposes.
\end_layout

\begin_layout Enumerate
Implement the RLOP algorithm and benchmark with other baseline solutions.
\end_layout

\begin_layout Enumerate
Investigate how the exterior parameters (such as interest rate, transaction
 cost) might play a role in the pricing strategy.
 
\end_layout

\begin_layout Standard
The learning algorithm we plan to use are MC methods and TD (or TD
\begin_inset Formula $\left(\lambda\right)$
\end_inset

) methods.
 The baseline we wish to compare against are the L-S model 
\begin_inset CommandInset citation
LatexCommand cite
key "longstaff2001valuing"
literal "false"

\end_inset

 and the QLBS method 
\begin_inset CommandInset citation
LatexCommand cite
key "halperin2020qlbs"
literal "false"

\end_inset

.
 We wish to further investigate which model best summaries the market behavior
 if allowed within the timeframe.
\end_layout

\begin_layout Standard
\begin_inset Note Comment
status collapsed

\begin_layout Section*
Video Transcript
\end_layout

\begin_layout Plain Layout
Hello! It's my pleasure to introduce our recent project on RL methods in
 option pricing.
 We aim to build a set of environments that captures the essence of the
 underlying financial insight while enabling machine learning in this regime.
 To begin with, option is a type of derivative contract, that gives the
 holder the right so that one can choose to buy or sell a certain amount
 of asset at a specific time.
 There has been some very successful theories about option pricing, for
 example the celebrated Black-Scholes formula.
 The idea is to replicate the option payoff by managing a hedge portfolio
 which involves the underlying asset and a cash account.
 The Black-Scholes formula produces the optimal hedge position series 
\begin_inset Formula $u_{t}$
\end_inset

 so that the payoff is perfectly replicated at the terminal time.
 However, their theory relies on continuous rehedging and assumes zero trading
 costs, which is either impractical or too harsh.
\end_layout

\begin_layout Plain Layout
In general, option pricing is a quite challenging problem.
 One may fail to notice an improper hedging action until the terminal time,
 let alone taking all the other constraints into account, for example discrete
 time, transaction cost and lack of asset pricing models.
 A recent paper by Igor Halperin in 2017 leverages RL theory to deal with
 these issues where the the price of an option includes a cashflow part
 and a variance part.
 The cashflow part is from Black-Scholes formula, while the variance part
 captures the risk from a variety range of factors.
 Besides, this approach does not assume an explicit model for the underlying
 trajectory, which saves the trouble of learning the pricing model.
 With that being said, the QLBS approach is not a fully interactive RL problem
 since the optimal hedging strategy is solved analytically instead of learning
 from exploration.
\end_layout

\begin_layout Plain Layout
In this work, we modify the reward and value function so that it enables
 learning by policy-based methods.
 The cashflow part is adjusted by the remaining time to the terminal time
 and the risk part uses the standard deviation so that it is invariant under
 scaling.
 The agent, under the actor-critic framework, learns the price of the option
 as the negative optimal value function.
 We adopt the policy gradient method to train the agent via an online fashion.
 Through a few numerical experiments, we show how the risk aversion parameter
 and trading cost could influence the optimal price learnt.
 We also compare the optimal price learnt with the one predicted by the
 Black-Scholes formula to better understand the risk premium.
 More details are included in the report.
\end_layout

\begin_layout Plain Layout
We also propose a novel approach which examines the hedge portfolio and
 the option payoff at the terminal time.
 Since the reward may come in a sparse manner as the agent has no idea how
 it is performing before the terminal time, we stack a class of option replicati
on tasks together so that the agent learns how to price a family of options
 at the same time.
 This framework is more straightforward, but it is harder to incorporate
 the risk concerns.
 We hope that our work can motivate and inspire deeper research of RL in
 the finance field.
 Thank you.
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset CommandInset bibtex
LatexCommand bibtex
btprint "btPrintCited"
bibfiles "ref"
options "siam"

\end_inset


\end_layout

\end_body
\end_document
