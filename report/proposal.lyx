#LyX 2.3 created this file. For more info see http://www.lyx.org/
\lyxformat 544
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass article
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype false
\use_dash_ligatures true
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing onehalf
\use_hyperref true
\pdf_bookmarks true
\pdf_bookmarksnumbered false
\pdf_bookmarksopen false
\pdf_bookmarksopenlevel 1
\pdf_breaklinks false
\pdf_pdfborder false
\pdf_colorlinks false
\pdf_backref false
\pdf_pdfusetitle true
\papersize default
\use_geometry true
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\use_minted 0
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 3.5cm
\topmargin 4cm
\rightmargin 3.5cm
\bottommargin 4cm
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\is_math_indent 0
\math_numbering_side default
\quotes_style english
\dynamic_quotes 0
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Standard
\begin_inset CommandInset include
LatexCommand include
filename "math_shorthand.lyx"

\end_inset


\end_layout

\begin_layout Title
Proposal of 
\begin_inset Quotes eld
\end_inset

RLOP: Reinforcement Learning in Option Pricing
\begin_inset Quotes erd
\end_inset


\end_layout

\begin_layout Author
Ziheng Chen, Zhou Fang
\end_layout

\begin_layout Section
Introduction
\end_layout

\begin_layout Standard
In this project, we are going to price options by using reinforcement learning.
 To achieve this goal, we need to replicate the payoff of options at different
 times and under different situations by holding and selling the underlying
 stocks at different times and under different situations.
 Since the whole portfolio will be self-financing, and at each step, different
 hedging strategies will make different value trajectories.
 In other words, to price options, people need to hedge a short position
 of the options.
 Therefore, our question is a sequential problem.
 
\end_layout

\begin_layout Standard
Black-Scholes formula (B-S formula) is the first formula that tells people
 how to price European-style call options.
 The idea behind the B-S formula is that one can hedge a short position
 of call options of a certain stock by holding that stock and putting extra
 money at the bank to earn interest; see 
\begin_inset CommandInset citation
LatexCommand cite
key "shreve2004stochastic,alma991057973644906011"
literal "false"

\end_inset

 for more in-depth theory and computation.
 The problem with the B-S formula is people need to hedge short positions
 continuously, which means they should sell or buy stock, and put or borrow
 money from the bank at every moment.
 This is unrealistic, and costly for almost all investors.
 Most importantly, the prices given by the B-S formula differ greatly from
 than prices of options in real life.
 In addition, a sub-optimal hedging position at the current period will
 affect the cash and stock position in the next period.
 The accumulation of those errors will affect the final price profoundly.
\end_layout

\begin_layout Section
Literature Review
\end_layout

\begin_layout Standard
There have been vast works and literatures on option pricing.
 A few exercising strategies for American option are reviewed in 
\begin_inset CommandInset citation
LatexCommand cite
key "li2009learning"
literal "false"

\end_inset

.
 The state space is composed of asset price trajectories 
\begin_inset Formula $\left(S_{0},S_{1},\dots,S_{T}\right)$
\end_inset

 and an absorbing state 
\begin_inset Formula $\boldsymbol{e}$
\end_inset

 which serves as the destination after the option has been exercised.
 The action space simply contains two actions: hold and exercise.
 The only non-zero reward is given when the option has been exercised at
 the value specified by the option.
 The Q-function is derived as in classical RL problems and it is the quantity
 of interest.
 The first method, LSPI (LS policy iteration), combines LSTD (LS with TD
 update) and policy iteration together to achieve efficient learning.
 The second and third ones, FQI (fitted Q-iteration algorithm) and LSMC
 (Least squares Monte Carlo), take an DP approach where the exact exercising
 problem is solved at each trading moment; the only difference lies in that
 FDI takes a forward view in time while LSMC is backward, starting from
 the exercising time.
 In the same spirit, 
\begin_inset CommandInset citation
LatexCommand cite
key "fathan2021deep"
literal "false"

\end_inset

 investigates the efficiency and effectiveness of different parametrizaiton
 structures.
 Three algorithms, namely double deep Q-Learning (DDQN), categorical distributio
nal RL (C51), and implicit quantile networks (IQN) are compared where DDQN
 learns the optimal Q-value function while the other two algorithms try
 to learn the full distribution of the discounted reward.
 They are tested on empirical data as well as simulated geometric Brownian
 motion trajectories.
\end_layout

\begin_layout Standard
Different RL architectures can be deployed in this field as well.
 The Actor-Critic structure is used in 
\begin_inset CommandInset citation
LatexCommand cite
key "marzban2021deep"
literal "false"

\end_inset

 for Equal Risk Pricing (ERP) in a risk averse setting under the framework
 studied in 
\begin_inset CommandInset citation
LatexCommand cite
key "tamar2015policy"
literal "false"

\end_inset

.
 The concept of 
\begin_inset Formula $\tau$
\end_inset

-expectile 
\begin_inset Formula 
\[
\overline{\rho}\left(X\right)=\arg\min_{q}\tau\bE\left[\left(q-X\right)_{+}^{2}\right]+\left(1-\tau\right)\bE\left[\left(q-X\right)_{-}^{2}\right]
\]

\end_inset

is used to elicit a coherent risk measure.
 The value function is defined as the portfolio value under the recursive
 coherent risk measure realized by expectiles, i.e.
\begin_inset Formula 
\[
V_{t}\left(S_{t},Y_{t}\right)=\inf_{\xi_{t}}\overline{\rho}\left(-\xi_{t}^{T}\Delta S_{t+1}+V_{t+1}\left(S_{t+1},Y_{t+1}\right)|S_{t},Y_{t}\right)
\]

\end_inset

with terminal condition 
\begin_inset Formula $V_{T}\left(S_{T},Y_{T}\right)=F\left(S_{T},Y_{T}\right)$
\end_inset

 specified by the option contract.
 With that being established, we can apply the policy gradient method where
 the critic network updates the value estimate which the actor network can
 refer to and build its policy upon.
 The network uses a classical fully multilayer structure with alternating
 activation functions.
\end_layout

\begin_layout Standard
A hybrid attempt is carried out in 
\begin_inset CommandInset citation
LatexCommand cite
key "grassl2010reinforcement"
literal "false"

\end_inset

 where the pricing strategy is based on a combination of optimal stopping
 and terminal payoff.
 The idea is that the agent can either hold the derivative until the terminal
 time, executing the contract to get the payoff written, or sell the derivative
 earlier according to the price at that particular moment.
 The reward function is thus simplified defined as the selling/execution
 price if such scenario happens.
 The value function is parameterized by kernel function approximation and
 the algorithm is tested using simulated geometric Brownian paths of an
 European call option.
\end_layout

\begin_layout Standard
We point out that it is also possible to directly solve the HJB equation
 if the associated RL problem is formulated as a control porblem.
 We refer to 
\begin_inset CommandInset citation
LatexCommand cite
key "halperin2021distributional"
literal "false"

\end_inset

 for more details on distributional offline continuous-time RL learning
 algorithms.
\end_layout

\begin_layout Section
Proposed plan
\end_layout

\begin_layout Standard
Based on the excellent QLBS paper 
\begin_inset CommandInset citation
LatexCommand cite
key "halperin2020qlbs"
literal "false"

\end_inset

, we propose to price options in discrete time and also consider the effects
 of transaction cost and interests rates difference for borrowing and depositing
 money at a bank.
 Consider a sequence of asset prices 
\begin_inset Formula $\left\{ S_{t}\right\} _{t=0,1,\dots}$
\end_inset

, adapted under the filtration 
\begin_inset Formula $\left\{ \cF_{t}\right\} $
\end_inset

, upon which we wish to build an option with payoff function 
\begin_inset Formula $h$
\end_inset

 at the maturity time 
\begin_inset Formula $T$
\end_inset

.
 The option is realized as a hedge portfolio which consists of some holdings
 
\begin_inset Formula $u_{t}$
\end_inset

 of the underlying asset and the risk-free deposit 
\begin_inset Formula $B_{t}$
\end_inset

.
 Let 
\begin_inset Formula 
\[
\Pi_{t}:=u_{t}S_{t}+B_{t}
\]

\end_inset

denote the value of the portfolio at time 
\begin_inset Formula $t$
\end_inset

.
 To fulfill the option contract, the holding position is cleared at the
 terminal time 
\begin_inset Formula $T$
\end_inset

 and is fully converted into cash position, i.e.
\begin_inset Formula 
\[
\Pi_{T}=B_{T}=h\left(S_{T}\right).
\]

\end_inset

The deposit position at a particular time is solved via the self-financing
 condition which requires that the instantaneous value of the portfolio
 is kept same before and after the re-hedging operation:
\begin_inset Formula 
\begin{equation}
u_{t}S_{t+1}+e^{r\Delta t}B_{t}=u_{t+1}S_{t+1}+B_{t+1}\label{eq:self-financing}
\end{equation}

\end_inset

where 
\begin_inset Formula $r$
\end_inset

 stands for the risk-free interest rate.
 Eqn.
 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:self-financing"

\end_inset

 can be used to solve how much money is needed to cover future trading activitie
s.
\end_layout

\begin_layout Standard
A state is defined as a tuple of an integer and two real numbers 
\begin_inset Formula $\left(t,X_{t},\widehat{B}_{t}\right)$
\end_inset

, where 
\begin_inset Formula 
\[
X_{t}=-\left(\mu-\frac{\sigma^{2}}{2}\right)t+\log S_{t}
\]

\end_inset

is the compensated logarithm price and 
\begin_inset Formula $\widehat{B}_{t}$
\end_inset

 is the estimate of the current deposit position.
 The action space is the real number, indicating how much shares are hedged
 at a particular time.
 A policy is a mapping from the state space to the action space, i.e.
\begin_inset Formula 
\[
\pi:\left(t,X_{t},\widehat{B}_{t}\right)\mapsto a_{t}.
\]

\end_inset

Notice that we use 
\begin_inset Formula $a_{t}$
\end_inset

 for the log-processed input 
\begin_inset Formula $X_{t}$
\end_inset

 while 
\begin_inset Formula $u_{t}$
\end_inset

 for the asset price 
\begin_inset Formula $S_{t}$
\end_inset

 in normal scale.
\end_layout

\begin_layout Standard
The reward function is derived from the Bellman's optimality equation where
 we define the value function in the first place.
 The idea is to minimize the money needed to initiate the hedge portfolio
 as well as to minimize the volatility throughout the trading periods.
 Given a hedging strategy 
\begin_inset Formula $\pi$
\end_inset

, the value function is defined as
\begin_inset Formula 
\[
V_{t}^{\pi}\left(X_{t}\right)=\bE^{\pi}\left[-\Pi_{t}\left(X_{t}\right)-\lambda\sum_{\tau=t}^{T}e^{-r\left(\tau-t\right)}\text{Var}\left[\Pi_{\tau}\left(X_{\tau}\right)|\cF_{\tau}\right]|\cF_{t}\right]
\]

\end_inset

where 
\begin_inset Formula $\lambda$
\end_inset

 is the risk aversion factor.
 The reward function can be derived by matching the corresponding terms
 in the Bellman's equation:
\begin_inset Formula 
\[
R_{t}\left(X_{t},a_{t},X_{t+1}\right):=\gamma a_{t}\Delta S_{t}-\lambda\text{Var}\left[\Pi_{t}|\cF_{t}\right]
\]

\end_inset

where 
\begin_inset Formula $\gamma=e^{-r\Delta t}$
\end_inset

 is the discounting factor.
 The reward function can be further complicated by the consideration of
 trading cost, usually adding an 
\begin_inset Formula $-\abs{a_{t}}$
\end_inset

 term.
 The connection between the value function and option pricing is that the
 option price is given by the minus optimal Q-function.
\end_layout

\begin_layout Standard
Based on the aforementioned settings proposed by QLBS 
\begin_inset CommandInset citation
LatexCommand cite
key "halperin2020qlbs"
literal "false"

\end_inset

, we seek to build a method where the RL agent can learn an optimal hedging
 strategy in a realistic setting.
 (We need to point out that QLBS is not an interactive RL model since the
 optimal hedging strategy 
\begin_inset Formula $\pi^{*}$
\end_inset

 is solved in a closed form rather than learning from the reward feedback).
 The initial plan is as follows:
\end_layout

\begin_layout Enumerate
Understand the financial background as well as finalize the mathematical
 standing point.
\end_layout

\begin_layout Enumerate
Implement a hedging environment as well as collect data for training purposes.
\end_layout

\begin_layout Enumerate
Implement the RLOP algorithm and benchmark with other baseline solutions.
\end_layout

\begin_layout Enumerate
Investigate how the exterior parameters (such as interest rate, transaction
 cost) might play a role in the pricing strategy.
 
\end_layout

\begin_layout Standard
The learning algorithm we plan to use are MC methods and TD (or TD
\begin_inset Formula $\left(\lambda\right)$
\end_inset

) methods.
 The baseline we wish to compare against are the L-S model 
\begin_inset CommandInset citation
LatexCommand cite
key "longstaff2001valuing"
literal "false"

\end_inset

 and the QLBS method 
\begin_inset CommandInset citation
LatexCommand cite
key "halperin2020qlbs"
literal "false"

\end_inset

.
 We wish to further investigate which model best summaries the market behavior
 if allowed within the timeframe.
\end_layout

\begin_layout Standard
\begin_inset CommandInset bibtex
LatexCommand bibtex
btprint "btPrintCited"
bibfiles "ref"
options "siam"

\end_inset


\end_layout

\end_body
\end_document
